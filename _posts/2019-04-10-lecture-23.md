---
layout: distill
title: "Lecture 23: Bayesian non-parametrics (continued)"
description: An example of a distill-style lecture notes that showcases the main elements.
date: 2019-04-10

lecturers:
  - name: Maruan Al-Shedivat
    url: "https://www.cs.cmu.edu/~mshediva/"

authors:
  - name: Cormac OMeadhra
    url: "#"
  - name: Sumit Kumar
    url: "#"
  - name: Author 3
    url: "#"
  - name: Author 4
    url: "#"

editors:
  - name: Lisa Lee
    url: "http://leelisa.com"

---

<!--Recap: Chinese restaurant process, stick breaking construction-->
## Recap

<!--Inference in the Dirichlet Process: Gibbs Samplers - Collapsed, Block, Slice-->
## Inference in the Dirichlet Process
<d-cite key="ishwaran2001gibbs">Gibbs Samplers</d-cite>
<d-cite key="hopper2015dpnotes">Dirichlet Process Notes</d-cite>

Given a Dirichlet process mixture model and a dataset that presumably has been generated from the model, inference is the process of determining the component of the mixture model from which each of the data point came from. Let us consider a Dirichlet process mixture model: 


$$\begin{aligned} 
G := \sum_{k=1}^{\infty} \pi_{k} \delta_{\theta_{k}} & \sim \operatorname{DP}(\alpha, H) \\
\phi_{n} & \sim G \\
x_{n} & \sim f\left(\phi_{n}\right)
\end{aligned}$$

where the data point $x_{n}$ is generated from the component $\phi_{n}$ of the mixture model $G$. We want to determine the component $\phi_{n}$ for each data point $x_{n}$. 

### Collapsed Gibbs Sampler
The collapsed Gibbs Sampler integrates out $G$ to get a Chinese restaurant process (CRP). Since, samples in a CRP are exchangeable, we can always rearrange the ordering of data points so that any sampled data point $x_{n}$ is the last one. Let, $z_{n}$ be the cluster allocation of the $x_{n}$ and $K$ be the total number of instantiated clusters, then the probability of $x_{n}$ belonging to the $k^{th}$ cluster is given by:

Note that, there is always a non-zero probability of the data point $x_{n}$ not being associated to any of the $K$ existing clusters and leads to the instantiation of a new cluster. 

However, there are a few problems with this approach:

- Updating only one data point at a time makes the algorithm infeasible for large datasets. 
- In the case of two true clusters getting merged into a single cluster in the beginning (possibly due to bad initialization), it is unlikely that a single data point will break out and form a different cluster.
- Getting to the true distribution involves going through low probability states and mixing can often be very slow.
- Integrating out parameter for new features is difficult if the likelihood is not conjugate and requires approximation of the integration. 

### Blocked Gibbs Sampler
In the Blocked Gibbs Sampler, we instantiate $G$ instead of integrating it out. Although $G$ is infinite-dimensional, we can simply approximate it with a truncated stick-breaking process:

$$\begin{aligned}
G^{K} : &= \sum_{k=1}^{K} \pi_{k} \delta_{\theta_{k}} \\
\pi_{k} &=b_{k} \prod_{j=1}^{k-1}\left(1-b_{j}\right) \\
b_{k} & \sim \operatorname{Beta}(1, \alpha) \text { for } k=1, \ldots, K-1 \\
b_{K} &= 1 
\end{aligned}$$

Now, for any $x_{n}$, its assocation probability to the $k^{th}$ cluster can be computed as:

$$
p(z_{n}|\text{rest}) \propto \pi_{k} f(w_{n}|\theta_{k})
$$

To estimate $\pi_{k}$ we follow the stick-breaking process which can also be thought of as a sequence of binary decisions. For example, we select $z_{n}=1$ with probability $b_{1}$. If $z_{n} \neq 1$, then we select $z_{n}=2$ with probability $b_{2}$ and so on. Formally, 

$$
b_{k} | \text{rest} \sim \operatorname{Beta} \left(1+m_{k},\alpha+\sum_{j=k+1}^{K} m_{j}\right)
$$

Unlike the Collapsed Gibbs Sampler, where we instantiate a new cluster with some non-zero probability, here we fix the maximum number of clusters ($K$) in the beginning only. This fixed truncation introduces some error in the inference. 


### Slice Sampler 
In the Slice Sampler, we introduce random truncation in place of pre-determined fixed truncation as in Blocked Gibbs Sampler. By marginalizing out this random truncation, we recover our original Dirichlet process. 

We introduce a random variable $u_{n}$ for each data point. The indicator $z_{n}$ is then sampled as follows:

$$ p\left(z_{n}=k | \mathrm{rest}\right)= I\left(\pi_{k}>u_{n}\right) f\left(x_{n} | \theta_{k}\right) $$

where, $I\left(\pi_{k}>u_{n}\right)$ is an indicator function which will select only a finite number of possible clusters those with $\pi_{k} > u_{n}$. The conditional distribution for $u_{n}$ is a uniform distribution with range $0$ to $\pi_{z_{n}}$: 

$$ u_{n} | rest \sim \operatorname{Uniform}[0, \pi_{z_{n}}]$$ 

Conditioned on $u_{n}$ and cluster indicator $z_{n}$, $\pi_{k}$ can be sampled according to the Blocked Gibbs Sampler. 

Here, we only need to represent a finite number of components $K$ such that 

$$ 1 - \sum_{k=1}^{K} \pi_{k} < min(u_{k}) $$

Slice sampler preserves the structure of blocked sampling, albeit the blocks are different for different points. Since, we do not have to integrate out $G$, it is much faster than Collapsed Gibbs Sampler. 

<!--Topic Models: LDA, HDP-->
## Topic Models
<d-cite key="teh2005sharing">Hierarchical Dirichlet Process</d-cite>

<!--Latent Variable Models: Indian Buffet Process-->
## Latent Variable Models
<d-cite key="griffiths2011indian">Indian Buffet Process</d-cite>
