---
layout: distill
title: "Lecture 23: Bayesian non-parametrics (continued)"
description: An example of a distill-style lecture notes that showcases the main elements.
date: 2019-04-10

lecturers:
  - name: Maruan Al-Shedivat
    url: "https://www.cs.cmu.edu/~mshediva/"

authors:
  - name: Cormac OMeadhra
    url: "#"
  - name: Sumit Kumar
    url: "#"
  - name: Wentao Yuan
    url: "#"

editors:
  - name: Lisa Lee
    url: "http://leelisa.com"

---

<!--Recap: Chinese restaurant process, stick breaking construction-->
## Recap

<!--Inference in the Dirichlet Process: Gibbs Samplers - Collapsed, Block, Slice-->
## Inference in the Dirichlet Process
<d-cite key="ishwaran2001gibbs">Gibbs Samplers</d-cite>
<d-cite key="hopper2015dpnotes">Dirichlet Process Notes</d-cite>

Given a Dirichlet process mixture model and a dataset that presumably has been generated from the model, inference is the process of determining the component of the mixture model from which each of the data point came from. Let us consider a Dirichlet process mixture model: 


$$\begin{aligned} 
G := \sum_{k=1}^{\infty} \pi_{k} \delta_{\theta_{k}} & \sim \operatorname{DP}(\alpha, H) \\
\phi_{n} & \sim G \\
x_{n} & \sim f\left(\phi_{n}\right)
\end{aligned}$$

where the data point $x_{n}$ is generated from the component $\phi_{n}$ of the mixture model $G$. We want to determine the component $\phi_{n}$ for each data point $x_{n}$. 

### Collapsed Gibbs Sampler
The collapsed Gibbs Sampler integrates out $G$ to get a Chinese restaurant process (CRP). Since, samples in a CRP are exchangeable, we can always rearrange the ordering of data points so that any sampled data point $x_{n}$ is the last one. Let, $z_{n}$ be the cluster allocation of the $x_{n}$ and $K$ be the total number of instantiated clusters, then the probability of $x_{n}$ belonging to the $k^{th}$ cluster is given by:

Note that, there is always a non-zero probability of the data point $x_{n}$ not being associated to any of the $K$ existing clusters and leads to the instantiation of a new cluster. 

However, there are a few problems with this approach:

- Updating only one data point at a time makes the algorithm infeasible for large datasets. 
- In the case of two true clusters getting merged into a single cluster in the beginning (possibly due to bad initialization), it is unlikely that a single data point will break out and form a different cluster.
- Getting to the true distribution involves going through low probability states and mixing can often be very slow.
- Integrating out parameter for new features is difficult if the likelihood is not conjugate and requires approximation of the integration. 

### Blocked Gibbs Sampler
In the Blocked Gibbs Sampler, we instantiate $G$ instead of integrating it out. Although $G$ is infinite-dimensional, we can simply approximate it with a truncated stick-breaking process:

$$\begin{aligned}
G^{K} : &= \sum_{k=1}^{K} \pi_{k} \delta_{\theta_{k}} \\
\pi_{k} &=b_{k} \prod_{j=1}^{k-1}\left(1-b_{j}\right) \\
b_{k} & \sim \operatorname{Beta}(1, \alpha) \text { for } k=1, \ldots, K-1 \\
b_{K} &= 1 
\end{aligned}$$

Now, for any $x_{n}$, its assocation probability to the $k^{th}$ cluster can be computed as:

$$
p(z_{n}|\text{rest}) \propto \pi_{k} f(w_{n}|\theta_{k})
$$

To estimate $\pi_{k}$ we follow the stick-breaking process which can also be thought of as a sequence of binary decisions. For example, we select $z_{n}=1$ with probability $b_{1}$. If $z_{n} \neq 1$, then we select $z_{n}=2$ with probability $b_{2}$ and so on. Formally, 

$$
b_{k} | \text{rest} \sim \operatorname{Beta} \left(1+m_{k},\alpha+\sum_{j=k+1}^{K} m_{j}\right)
$$

Unlike the Collapsed Gibbs Sampler, where we instantiate a new cluster with some non-zero probability, here we fix the maximum number of clusters ($K$) in the beginning only. This fixed truncation introduces some error in the inference. 


### Slice Sampler 
In the Slice Sampler, we introduce random truncation in place of pre-determined fixed truncation as in Blocked Gibbs Sampler. By marginalizing out this random truncation, we recover our original Dirichlet process. 

We introduce a random variable $u_{n}$ for each data point. The indicator $z_{n}$ is then sampled as follows:

$$ p\left(z_{n}=k | \mathrm{rest}\right)= I\left(\pi_{k}>u_{n}\right) f\left(x_{n} | \theta_{k}\right) $$

where, $I\left(\pi_{k}>u_{n}\right)$ is an indicator function which will select only a finite number of possible clusters those with $\pi_{k} > u_{n}$. The conditional distribution for $u_{n}$ is a uniform distribution with range $0$ to $\pi_{z_{n}}$: 

$$ u_{n} | rest \sim \operatorname{Uniform}[0, \pi_{z_{n}}]$$ 

Conditioned on $u_{n}$ and cluster indicator $z_{n}$, $\pi_{k}$ can be sampled according to the Blocked Gibbs Sampler. 

Here, we only need to represent a finite number of components $K$ such that 

$$ 1 - \sum_{k=1}^{K} \pi_{k} < min(u_{k}) $$

Slice sampler preserves the structure of blocked sampling, albeit the blocks are different for different points. Since, we do not have to integrate out $G$, it is much faster than Collapsed Gibbs Sampler. 

<!--Topic Models: HDP-LDA-->
## Hierarchical Dirichlet Process
### Motivation: Topic Models
As introduced in previous lectures, a topic model is a hierarchical graphical model for document description. Under a topic model,
- Each document is a distribution over topics;
- Each topic is a distribution over words;
- Each document is an unordered collection of words (bag-of-words) sampled from the topics;
- The topics are *shared* among documents.

Latent Dirichlet Allocation (LDA) <d-cite key="blei2003latent"></d-cite> (Figure 1) is a particular kind of topic model where the distribution over topics and the distribution over words are both Dirichlet distributions. In LDA, the number of topics $K$ is specified in advance. This naturally leads to the non-trivial question of how to choose the correct number of topics. We have seen that Dirichlet process enables us to model a Gaussian mixture with an unbounded number of components. Can we apply the same idea here, replace the Dirichlet distribution over topics with a Dirichlet process, and get an LDA with unbounded number of topics?
<figure>
  <img src="{{ 'assets/img/notes/lecture-23/lda.png' | relative_url }}" />
  <figcaption>
    <strong>Figure 1.</strong>
    Graphical representation of Latent Dirichlet Allocation <d-cite key="blei2003latent"></d-cite>
  </figcaption>
</figure>

However, a naive implementation of Dirichlet process will fail in the case of topic models, because it cannot ensure the topics are shared. To be more specific, for a DP with a *continuous* base measure, we have zero probability of sampling the same atoms twice. As a result, a new set of topics will be generated every time we try to sample a document, making the topic model useless. Thus, we want the base measure to be discrete, but we don't want to pin down the number of possible topics as in the original LDA. In other words, we need an **infinite, discrete, random** base measure, which leads to the Hierarchical Dirichlet Process (HDP) model proposed by Teh et al. <d-cite key="teh2005sharing"></d-cite>

### Hierarchical Dirichlet Process
The key observation of Teh et al. <d-cite key="teh2005sharing"></d-cite> is that a sample from a Dirichlet process gives us exactly what we want -- an infinite, discrete, random base measure. Hence, we can formulate a *hierarchical* Dirichlet process, where
<d-math block>
\begin{aligned}
G_0 \mid \gamma,H &\sim \mathrm{DP}(\gamma,H) \\
G_j \mid \alpha_0,G_0 &\sim \mathrm{DP}(\alpha_0,G_0)
\end{aligned}
</d-math>
Here, $G_0$ is the global measure distributed as a Dirichlet process with concentration parameter $\gamma$ and base probability measure $H$. $G_j$ are the random measures which are conditionally independent given $G_0$, with distributions given by a Dirichlet process with base probability measure $G_0$. We can use $G_j$ to describe the distribution of topics for each document. Figure 2 shows a simple illustration of the sampling process of a HDP.
<figure>
  <img src="{{ 'assets/img/notes/lecture-23/hdp.png' | relative_url }}" />
  <figcaption>
    <strong>Figure 2.</strong>
    Sampling from a Hierarchical Dirichlet Process <d-cite key="teh2005sharing"></d-cite>
  </figcaption>
</figure>

As in the case of DP, we can interpret HDP using the Chinese Restaurant Process analogy, where instead of a single restaurant, we now have a franchise of restaurants (documents). The restaurants share a common menu of dishes (topics). Each customer (word) picks a table in the restaurant and each table orders a dish (topic) according to the shared menu.

The following process desribes how to sample from a HDP-LDA. For more details, please refer to the paper <d-cite key="teh2005sharing"></d-cite>.
- Sample the global measure $G_0\sim\mathrm{DP}(\gamma,H)$ (e.g. using stick breaking)
- For each document $j$
    - Sample a distribution over topics $G_j\sim\mathrm{DP}(\alpha_0,G_0)$
    - For $i=1,\dots,N_j$
        - Sample a topic $\phi_{ji}\sim G_j$
        - Sample a word $w_{ji}\sim P(\phi_{ji})$

Figure 3 shows the perplexity score (i.e. negative log likelihood) of LDAs trained with different number of topics compared with HDP-LDA. It can be seen that HDP-LDA achieves the lowest possible perplexity by automatically selecting the "right" number of topics.
<figure>
  <img src="{{ 'assets/img/notes/lecture-23/perplexity.png' | relative_url }}" />
  <figcaption>
    <strong>Figure 3.</strong>
    LDA vs. HDP-LDA
  </figcaption>
</figure>

<!--Latent Variable Models: Indian Buffet Process-->
## Latent Variable Models
<d-cite key="griffiths2011indian">Indian Buffet Process</d-cite>
