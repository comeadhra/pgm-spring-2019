---
layout: distill
title: "Lecture 23: Bayesian non-parametrics (continued)"
description: An example of a distill-style lecture notes that showcases the main elements.
date: 2019-04-10

lecturers:
  - name: Maruan Al-Shedivat
    url: "https://www.cs.cmu.edu/~mshediva/"

authors:
  - name: Cormac OMeadhra
    url: "#"
  - name: Author 2
    url: "#"
  - name: Author 3
    url: "#"
  - name: Author 4
    url: "#"

editors:
  - name: Lisa Lee
    url: "http://leelisa.com"

---

<!--Recap: Chinese restaurant process, stick breaking construction-->
## Recap

<!--Inference in the Dirichlet Process: Gibbs Samplers - Collapsed, Block, Slice-->
## Inference in the Dirichlet Process
<d-cite key="ishwaran2001gibbs">Gibbs Samplers</d-cite>
<d-cite key="hopper2015dpnotes">Dirichlet Process Notes</d-cite>

<!--Topic Models: LDA, HDP-->
## Topic Models
<d-cite key="teh2005sharing">Hierarchical Dirichlet Process</d-cite>

<!--Latent Variable Models: Indian Buffet Process-->
## Latent Variable Models
### Motivation
Latent class models introduced previously assume that classes are independent,
which prevents the sharing of features across classes. However, in many
applications it would be useful to share features across data points. For example,
- Images contain multiple objects which may have common features
- Individuals in social networks may belong to multiple social groups

Latent variable models introduce a latent feature space
$$ \mathbf{A} $$, from which the data can be constructed. For example,
in Factor Analyis, the data is assumed to be a linear combination of the features

$$ \mathbf{X} = \mathbf{W} \mathbf{A}^T + \varepsilon $$

However, in general, it is not necessary for $$ \mathbf{W} $$ and $$ \mathbf{A} $$
to be combined linearly.

We are interested in answering the following two questions:
- Can we make the number of features (rows of $$ \mathbf{A} $$, columns of $$ \mathbf{W} $$)
  unbounded?
- Can we do so in a manner that makes inference tractable?

We see that both of these properties can be achieved through the framework introduced
by the Indian Buffet Process (IBP) <d-cite key="griffiths2011yyindian">Indian Buffet Process</d-cite>.

### Sparse Latent Variable Models
We first consider the a large, but finite, sparse latent variable model.
The sparsisty property is desirable so that as the number of features grows large,
we can bound the expected number of non-zero features, which is a requirement for
a tractable inference procedure.

The Chinese Restaurant Process discussed previously can be considered as a distribution
over sparse finite bindary matrices, i.e. each customer (data point) is assigned
to a single table (feature), leading to a matrix with a single non-zero entry per row.

Such a representation is excessively sparse, which motivates allowing multiple active features
per row. In the restaurant analogy, this is equivalent to the customer sitting at multiple
tables (or in IBP, get multiple dishes) - more on this later.

Let's define our weight matrix as

$$ \mathbf{W} = \mathbf{Z} \odot \mathbf{V} $$

where $$ \mathbf{Z} $$ is a sparse binary matrix.

We are interested in defining a distribution $$ p(\mathbf{Z}) $$, which is achieved
by placing a *beta-Bernoulli* prior over $$ \mathbf{Z} $$.

$$ \pi_{k} \sim \text{Beta}\left(\frac{\alpha}{K}, 1\right), ~~ k=1,...,K $$

$$ z_{nk} \sim \text{Bernoulli}\left(\pi_{k}\right), ~~ n=1,...,N $$

Using the fact that the columns (features) of  $$ \mathbf{Z} $$ are
*i.i.d.* and the rows are *i.i.d* when conditioned on $$\pi_{k}$$, and
noting that the beta distribution is conjugate to the Bernoulli, the
distribution over $$ \mathbf{Z} $$ is given by

$$ p(\mathbf{Z}) = \prod_{k=1}^{K} \int \left(\prod_{n=1}^{N} p(z_{nk}| \pi_{k}) \right) p(\pi_{k}) d\pi_{k}
                 = \prod_{k=1}^{K} \frac{\alpha}{K} \frac{\Gamma(m_k + \alpha/K) \Gamma(N - m_{k} + 1)}
                                        {\Gamma(N + 1 + \alpha/K)}$$

Where $$ m_{k} = \sum_{n=1}^{N} z_{nk} $$ is the number of data points with active feature $$ k $$.

We can show that that this matrix is sparse by considering the expected number of non-zero entries

$$ \mathbb{E}_{\pi_{1}, ..., \pi_{K}}\left[ \mathbf{1}^T \mathbf{Z} \mathbf{1} \right]
   = K \mathbb{E}_{\pi_{k}}\left[ \mathbf{1}^T \mathbf{z}_k \right]
   = K \sum_{n=1}^N \mathbb{E}_{\pi_{k}}\left[ z_{nk}\right]
   = K \sum_{n=1}^N \int_{0}^{1} \pi_{k} p(\pi_{k}) \mathrm{d} \pi_{k} = \frac{N \alpha}{1 + \frac{\alpha}{K}} $$

which is bounded above by $$ N \alpha $$.

### Infinite Sparse Latent Variable Models
Taking $$ K \rightarrow \infty $$ in the model above would result in a matrices with infinite numbers of
empty columns. To get arrow this, define an equivalence class $$ [\mathbf{Z}] $$ of
*left-ordered* binary matrices given by the function $$ \mathit{lof}(\cdot) $$.
Specifically, the matrices are sorted in order of decreasing binary number defined by
considering the rows of a single column as bits.

<figure id="example-figure" class="l-body-outset">
  <div class="row">
  <div class="col two">
      <img src="{{ 'assets/img/notes/lecture-23/lof.png' | relative_url }}" />
  <figcaption>
    <strong>Binary matrix and the left ordered form. Taken from
            <d-cite key="griffiths2011yyindian">Indian Buffet Process</d-cite>.
    </strong>
  </figcaption>
  </div>
  </div>
</figure>

All matrices with the equivalence class $$ [\mathbf{Z}] $$ have equal probability.
The total probability of the class is then equal to $$| [\mathbf{Z} ] | p(\mathbf{Z}) $$,
where $$| [\mathbf{Z} ] | $$ is the number of matrices in the class. In order to compute
$$| [\mathbf{Z} ] | $$, we introduce a few terms
- Let the *history* of a feature $k$ be defined by the binary number equivalent of its rows,
  e.g. a 4 data point example with column $$(0, 1, 0, 1)$$ has history 5.
- Let $K_{h}$ denote the number of features with history $h$.
- Let $K_{+}$ denote the number of features with non-zero history.
- Let $K = K_{0} + K_{+}$, i.e. the sum of the zero history and non-zero history features.

For $N$ data points, we have a maximum history of $2^{N-1}$ (max N-bit binary number). Since
columns are exchangeable, we note that any permutation of columns with history $h$ does
not chage the matrix under $$\mathit{lof}$$ equivalence.

$$| [\mathbf{Z} ] | =  {K \choose {K_{0}...K_{2^{N}-1}}} = \frac{K!}{\prod_{h=0}^{2^{N}-1}K_h!}$$

The distribution over $$ [\mathbf{Z}] $$ is then


$$ p([\mathbf{Z}]) = |[\mathbf{Z}]|p(\mathbf{Z})
                 = \frac{K!}{\prod_{h=0}^{2^{N}-1}K_h!} \prod_{k=1}^{K}
                    \frac{\alpha}{K} \frac{\Gamma(m_k + \alpha/K) \Gamma(N - m_{k} + 1)}
                                        {\Gamma(N + 1 + \alpha/K)}$$

We can evaluate this term by splitting $$ K $$ into the terms for which $$ m_{k} = 0 $$ and
$$ m_{k} > 0 $$ (see eqn (13) in <d-cite key="griffiths2011yyindian">Indian Buffet Process</d-cite>)
<d-math block>
\begin{aligned}
p([\mathbf{Z}]) &= \left(\frac{\frac{\alpha}{K} \Gamma\left(\frac{\alpha}{K}\right) \Gamma(N+1)}
                               {\Gamma\left(N+1+\frac{\alpha}{K}\right)}\right)^{K-K_{+}}
                    \quad
                    \prod_{k=1}^{K_{+}} \frac{\frac{\alpha}{K} \Gamma\left(m_{k}+\frac{\alpha}{K}\right)
                                                               \Gamma\left(N-m_{k}+1\right)}
																						 {\Gamma\left(N+1+\frac{\alpha}{K}\right)} \\
								&= \left(\frac{\frac{\alpha}{K} \Gamma\left(\frac{\alpha}{K}\right) \Gamma(N+1)}
															{\Gamma\left(N+1+\frac{\alpha}{K}\right)}\right)^{K}
										\quad
										\prod_{k=1}^{K_{+}} \frac{\Gamma\left(m_{k}+\frac{\alpha}{K}\right)
																							\Gamma\left(N-m_{k}+1\right)}
																						 {\Gamma\left(\frac{\alpha}{K}\right) \Gamma(N+1)} \\
								&= \left(\frac{N !}{\prod_{j=1}^{N}\left(j+\frac{\alpha}{K}\right)}\right)^{K}
										\quad
										\left(\frac{\alpha}{K}\right)^{K_{+}}
													\prod_{k=1}^{K_{+}}
													\frac{\left(N-m_{k}\right) ! \prod_{j=1}^{m_{k}-1}\left(j+\frac{\alpha}{K}\right)}{N!}
\end{aligned}
</d-math>

Taking the limit as $$K \rightarrow \infty$$


<d-math block>
\begin{aligned}
\lim_{K \rightarrow \infty} p([\mathbf{Z}]) &= \lim_{K \rightarrow \infty} \frac{\alpha^{K_{+}}}{\prod_{h=1}^{2^{N}-1} K_{h} !}
																							 \cdot \frac{K !}{K_{0} ! K^{K_{+}}}
                                               \cdot\left(\frac{N!}{\Pi_{j=1}^{N}\left(j+\frac{\alpha}{K}\right)}\right)^{K}
                                               \cdot \prod_{k=1}^{K_{+}}
                                               \frac{\left(N-m_{k}\right) ! \Pi_{j=1}^{m_{k}-1}\left(j+\frac{\alpha}{K}\right)}{N !} \\
																						&= \quad \quad
                                               \frac{\alpha^{K_{+}}}{\prod_{h=1}^{2^{N}-1} K_{h} !}
                                               \cdot \quad \quad 1 \quad \quad
                                               \cdot \quad \exp \left\{-\alpha H_{N}\right\} \quad
                                               \cdot \prod_{k=1}^{K_{+}} \frac{\left(N-m_{k}\right) !\left(m_{k}-1\right) !}{N !}
\end{aligned}
</d-math>
where $$H_{N} = \sum_{j=1}^{N} \frac{1}{j}$$ is the $N$th harmonix number. See Appendix A in <d-cite key="griffiths2011yyindian"></d-cite>
for more details about this limit.

### Intuition: Indian Buffet Process

**Setup**: Buffet with infinite number of dishes. Customers can select any number of dishes.
**Procedure**:
- Customer enters restaurant and selects $$ \text{Poisson}(\alpha) $$ dishes
- The $$n$$th customer considers each previously selected dish and selects from it with probability $$ \frac{m_{k}}{n} $$.
- The $$n$$th customer then selects $$ \text{Poisson}(\frac{\alpha}{n}) $$ new dishes

We can show that this sequential process is *lof*-equivalent to the infinite *beta-Bernoulli* model defined
previously.


<d-math block>
\begin{aligned}
p(\mathbf{Z}) &= \prod^{N}_{n=1} p\left(\mathbf{z}_{n} | \mathbf{z}_{1 :(n-1)}\right) \\
              &=\prod_{n=1}^{N} \operatorname{Poisson}\left(K_{1}^{(n)} | \frac{\alpha}{n}\right)
								\prod_{k=1}^{K_{+}}\left(\frac{\sum_{i=1}^{n-1} z_{i k}}{n}\right)^{z_{n k}}
								\left(\frac{n-\sum_{i=1}^{n-1} z_{i k}}{n}\right)^{1-z_{n k}} \\
              &=\prod_{n=1}^{N}\left(\frac{\alpha}{n}\right)^{K_{1}^{(n)}}
                \frac{1}{K_{1}^{(n)} !} e^{-\alpha / n}
                \prod_{k=1}^{K_{+}}\left(\frac{\sum_{i=1}^{n-1} z_{i k}}{n}\right)^{z_{n k}}
                                   \left(\frac{n-\sum_{i=1}^{n-1} z_{i k}}{n}\right)^{1-z_{n k}}\\
							&=\frac{\alpha^{K_{+}}}{\prod_{n=1}^{N} K_{1}^{(n)} !}
                \exp \left\{-\alpha H_{N}\right\}
                \prod_{k=1}^{K_{+}} \frac{ ( N-m_{k} ) !\left(m_{k}-1\right) !}{N !}
\end{aligned}
</d-math>

where $$K_{1}^{(n)}$$ is the number of new features in the $$n$$th row. Accounting for the
cardinality of the *lof*-equivalence set gives the same result as the *beta-Bernoulli* case.

### Summary
The IBP enables reasoning about a potentially infinite number of features, which can be selected in a data-dependent
manner. The capacity of the model (in terms of # of features) does not need to be specified *a priori*, but can be
controlled through a prior distribution over the feature selection matrices.
Some properties of the IBP
- "Rich get richer" - popular dishes (features selected by many data points) become more popular (are assigned to more data).
- The number of dishes selected by a customer (non-zero entries in each data row)
  is distributted according to $$ \text{Poisson}(\alpha) $$
- The number of dishes selected by **all** customers (number of non-zero entries) is distributed according to $$ \text{Poisson}(N \alpha) $$.
- The number of dishes selected by **any** customer (number of non-zero columns) is distributed according to $$ \text{Poisson}(\alpha H_{n}) $$.
- The total number of dishes selected (number of non-zero entries) in expectation is bounded by $$ N \alpha $$
